import os
import chainlit as cl
from openai import OpenAI

# ========================
# CONFIG GERAIS
# ========================
MCP_URL = "https://solutions-garage-ai-gateway-lab.sensedia-eng.com/insurance-mcp/v1/mcp"
MCP_LABEL = "mcp-insurance"

OPENAI_MODEL = "gpt-5.1"#"gpt-5-nano"#"gpt-5.1"  # pode trocar p/ outro modelo compat√≠vel
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MCP_TOKEN = os.getenv("MCP_TOKEN")

client = OpenAI(api_key=OPENAI_API_KEY)


def call_llm_with_mcp(user_message: str) -> str:
    """
    Chama o modelo da OpenAI usando o MCP server mcp-insurance como tool.
    Quem fala "protocolo MCP + streamable_http" com teu servidor √© a OpenAI,
    n√£o o Chainlit diretamente.
    """

    if not OPENAI_API_KEY:
        return (
            "‚ö†Ô∏è Falta configurar OPENAI_API_KEY no ambiente.\n"
            "Use: export OPENAI_API_KEY='sua_chave_aqui'"
        )

    if not MCP_TOKEN:
        return (
            "‚ö†Ô∏è Falta configurar MCP_TOKEN no ambiente.\n"
            "Use: export MCP_TOKEN='seu_jwt_aqui'"
        )

    try:
        resp = client.responses.create(
            model=OPENAI_MODEL,
            tools=[
                {
                    "type": "mcp",
                    "server_label": MCP_LABEL,
                    "server_description": "MCP de seguros da Sensedia",
                    "server_url": MCP_URL,
                    # Token que o MCP precisa para autentica√ß√£o/autoriza√ß√£o
                    "authorization": MCP_TOKEN,
                    # Para demo: deixa o modelo chamar o MCP sem pedir aprova√ß√£o
                    "require_approval": "never",
                }
            ],
            input=user_message,
        )
    except Exception as e:
        return f"‚ùå Erro ao chamar OpenAI + MCP: {e}"

    # A Responses API j√° faz as chamadas ao MCP e monta a resposta final.
    try:
        return resp.output_text  # texto pronto para exibir ao usu√°rio
    except Exception:
        return f"üì¶ Resposta bruta do modelo: {resp}"
    

@cl.on_chat_start
async def on_chat_start():
    await cl.Message(
        content=(
            "‚úÖ Chat conectado ao LLM + MCP `mcp-insurance`.\n\n"
            "Voc√™ pode fazer perguntas de neg√≥cio (seguros) e o modelo decide "
            "quando chamar o MCP."
        )
    ).send()


@cl.on_message
async def on_message(message: cl.Message):
    user_text = message.content
    reply = call_llm_with_mcp(user_text)
    await cl.Message(content=reply).send()
